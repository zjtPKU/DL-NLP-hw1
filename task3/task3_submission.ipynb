{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function, division\n",
    "from io import open\n",
    "import unicodedata\n",
    "import re\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Preprocessing\n",
    "from sudachipy import dictionary\n",
    "tokenizer = dictionary.Dictionary().create()\n",
    "\n",
    "SOS_token = 0\n",
    "EOS_token = 1\n",
    "UNK_token = 2\n",
    "class Lang:\n",
    "    def __init__(self, name):\n",
    "        self.name = name\n",
    "        self.word2index = {\"SOS\":0,\"EOS\":1,\"UNK\":2}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"UNK\"}\n",
    "        self.n_words = 2  # Count SOS and EOS\n",
    "\n",
    "    def addSentence(self, sentence,type=\"eng\"):\n",
    "        if type == \"eng\":\n",
    "            for word in sentence.split(' '):\n",
    "                self.addWord(word)\n",
    "        elif type==\"jpn\":\n",
    "            tokens = [m.surface() for m in tokenizer.tokenize(sentence)]\n",
    "            for token in tokens:\n",
    "                self.addWord(token)\n",
    "    def addWord(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2index[word] = self.n_words\n",
    "            self.word2count[word] = 1\n",
    "            self.index2word[self.n_words] = word\n",
    "            self.n_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Some filters for English dataset; But I dont know japanese!!!!!\n",
    "\n",
    "def unicodeToAscii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn'\n",
    "    )\n",
    "\n",
    "# Lowercase, trim, and remove non-letter characters\n",
    "def normalizeString(s):\n",
    "    s = unicodeToAscii(s.lower().strip())\n",
    "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
    "    s = re.sub(r\"[^a-zA-Z!?]+\", r\" \", s)\n",
    "    return s.strip()\n",
    "\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "eng_prefixes = (\n",
    "    \"i am \", \"i m \",\n",
    "    \"he is\", \"he s \",\n",
    "    \"she is\", \"she s \",\n",
    "    \"you are\", \"you re \",\n",
    "    \"we are\", \"we re \",\n",
    "    \"they are\", \"they re \"\n",
    ")\n",
    "\n",
    "def filterPair(p):\n",
    "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
    "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
    "        p[1].startswith(eng_prefixes)\n",
    "\n",
    "\n",
    "def filterPairs(pairs):\n",
    "    return [pair for pair in pairs if filterPair(pair)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make the vocabulary and do some statistics\n",
    "\n",
    "def readLangs(lang1, lang2, reverse=False):\n",
    "    print(\"Reading lines...\")\n",
    "\n",
    "    # Read the file and split into lines\n",
    "    # lines = []\n",
    "    with open('eng_jpn.txt', 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    pairs = []\n",
    "    # Split every line into pairs and normalize\n",
    "    for line in lines:\n",
    "        pair_raw = line.strip().split('\\t')\n",
    "        pair_raw[1] = normalizeString(pair_raw[1])\n",
    "        # print(pair_raw)\n",
    "        pairs.append(pair_raw)\n",
    "    \n",
    "    # Reverse pairs, make Lang instances\n",
    "    if reverse:\n",
    "        pairs = [list(reversed(p)) for p in pairs]\n",
    "        input_lang = Lang(lang2)\n",
    "        output_lang = Lang(lang1)\n",
    "    else:\n",
    "        input_lang = Lang(lang1)\n",
    "        output_lang = Lang(lang2)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n",
    "\n",
    "def prepareData(lang1, lang2, reverse=False):\n",
    "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
    "    print(\"Read %s sentence pairs\" % len(pairs))\n",
    "    # pairs = filterPairs(pairs)\n",
    "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
    "    print(\"Counting words...\")\n",
    "    for pair in pairs:\n",
    "        input_lang.addSentence(pair[0],\"jpn\")\n",
    "        output_lang.addSentence(pair[1],\"eng\")\n",
    "    print(\"Counted words:\")\n",
    "    print(input_lang.name, input_lang.n_words)\n",
    "    print(output_lang.name, output_lang.n_words)\n",
    "\n",
    "    return input_lang, output_lang, pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "input_lang, output_lang, pairs = prepareData('Jpn', 'Eng')\n",
    "print(random.choice(pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the train/valid/test dataloader\n",
    "\n",
    "def indexesFromSentence(lang, sentence,type=\"jpn\"):\n",
    "    if type == \"eng\":\n",
    "        return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "    elif type==\"jpn\":\n",
    "        tokens = [m.surface() for m in tokenizer.tokenize(sentence)]\n",
    "        return [lang.word2index[token] if token in lang.word2index else lang.word2index[\"UNK\"] for token in tokens]\n",
    "    # return [lang.word2index[word] for word in sentence.split(' ')]\n",
    "\n",
    "def tensorFromSentence(lang, sentence,type=\"jpn\"):\n",
    "    indexes = indexesFromSentence(lang, sentence,type)\n",
    "    indexes.append(EOS_token)\n",
    "    return torch.tensor(indexes, dtype=torch.long, device=device).view(1, -1)\n",
    "\n",
    "def tensorsFromPair(pair):\n",
    "    input_tensor = tensorFromSentence(input_lang, pair[0],\"jpn\")\n",
    "    target_tensor = tensorFromSentence(output_lang, pair[1],\"eng\")\n",
    "    return (input_tensor, target_tensor)\n",
    "\n",
    "def get_dataloader(batch_size):\n",
    "    input_lang, output_lang, pairs = prepareData('jpn', 'eng')\n",
    "\n",
    "    valid_input_ids = []\n",
    "    valid_target_ids = []\n",
    "\n",
    "    for inp, tgt in pairs:\n",
    "        inp_ids = indexesFromSentence(input_lang, inp, \"jpn\")\n",
    "        tgt_ids = indexesFromSentence(output_lang, tgt, \"eng\")\n",
    "        \n",
    "        if len(inp_ids) > MAX_LENGTH - 1 or len(tgt_ids) > MAX_LENGTH - 1 or len(inp_ids) == 0 or len(tgt_ids) == 0:\n",
    "            print(f\"Skipped: {inp} -> {tgt}\")\n",
    "            continue\n",
    "        # 添加 EOS_token\n",
    "        inp_ids.append(EOS_token)\n",
    "        tgt_ids.append(EOS_token)\n",
    "        # 填充到 MAX_LENGTH\n",
    "        inp_ids_padded = np.pad(inp_ids, (0, MAX_LENGTH - len(inp_ids)), mode='constant', constant_values=0)\n",
    "        tgt_ids_padded = np.pad(tgt_ids, (0, MAX_LENGTH - len(tgt_ids)), mode='constant', constant_values=0)\n",
    "        \n",
    "        valid_input_ids.append(inp_ids_padded)\n",
    "        valid_target_ids.append(tgt_ids_padded)\n",
    "\n",
    "    valid_input_ids = np.array(valid_input_ids, dtype=np.int32)\n",
    "    valid_target_ids = np.array(valid_target_ids, dtype=np.int32)\n",
    "\n",
    "    n = len(valid_input_ids) \n",
    "    print(\"number of valid data: \", n)\n",
    "    train_size = int(0.8 * n)\n",
    "    valid_size = int(0.1 * n)\n",
    "    test_size = n - train_size - valid_size \n",
    "\n",
    "    input_tensor = torch.LongTensor(valid_input_ids)\n",
    "    target_tensor = torch.LongTensor(valid_target_ids)\n",
    "\n",
    "    train_data = TensorDataset(input_tensor[:train_size].to(device), target_tensor[:train_size].to(device))\n",
    "    valid_data = TensorDataset(input_tensor[train_size:train_size + valid_size].to(device), target_tensor[train_size:train_size + valid_size].to(device))\n",
    "    test_data = TensorDataset(input_tensor[train_size + valid_size:].to(device), target_tensor[train_size + valid_size:].to(device))\n",
    "\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    valid_sampler = RandomSampler(valid_data)\n",
    "    test_sampler = RandomSampler(test_data)\n",
    "\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "    valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_size)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "\n",
    "    return input_lang, output_lang, train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Encoder architecture\n",
    "import torch.nn as nn\n",
    "\n",
    "class EncoderRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, dropout_p=0.1):\n",
    "        super(EncoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "        output, (hidden, cell) = self.lstm(embedded)  \n",
    "        return output, (hidden, cell)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Decoder architecture\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.Wa = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Ua = nn.Linear(hidden_size, hidden_size)\n",
    "        self.Va = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, query, keys):\n",
    "        scores = self.Va(torch.tanh(self.Wa(query) + self.Ua(keys)))\n",
    "        scores = scores.squeeze(2).unsqueeze(1)\n",
    "\n",
    "        weights = F.softmax(scores, dim=-1)\n",
    "        context = torch.bmm(weights, keys)\n",
    "\n",
    "        return context, weights\n",
    "\n",
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
    "        self.attention = BahdanauAttention(hidden_size)\n",
    "        self.lstm = nn.LSTM(2 * hidden_size, hidden_size, batch_first=True)\n",
    "        self.out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout_p)\n",
    "\n",
    "    def forward(self, encoder_outputs, encoder_hidden, target_tensor=None):\n",
    "        batch_size = encoder_outputs.size(0)\n",
    "        decoder_input = torch.empty(batch_size, 1, dtype=torch.long, device=device).fill_(SOS_token)\n",
    "        decoder_hidden = encoder_hidden  # encoder_hidden should contain (hidden, cell) for LSTM\n",
    "        decoder_outputs = []\n",
    "        attentions = []\n",
    "\n",
    "        for i in range(MAX_LENGTH):\n",
    "            decoder_output, decoder_hidden, attn_weights = self.forward_step(\n",
    "                decoder_input, decoder_hidden, encoder_outputs\n",
    "            )\n",
    "            decoder_outputs.append(decoder_output)\n",
    "            attentions.append(attn_weights)\n",
    "\n",
    "            if target_tensor is not None:\n",
    "                # Teacher forcing: Feed the target as the next input\n",
    "                decoder_input = target_tensor[:, i].unsqueeze(1)  # Teacher forcing\n",
    "            else:\n",
    "                # Without teacher forcing: use its own predictions as the next input\n",
    "                _, topi = decoder_output.topk(1)\n",
    "                decoder_input = topi.squeeze(-1).detach()  # detach from history as input\n",
    "\n",
    "        decoder_outputs = torch.cat(decoder_outputs, dim=1)\n",
    "        decoder_outputs = F.log_softmax(decoder_outputs, dim=-1)\n",
    "        attentions = torch.cat(attentions, dim=1)\n",
    "\n",
    "        return decoder_outputs, decoder_hidden, attentions\n",
    "\n",
    "    def forward_step(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.dropout(self.embedding(input))\n",
    "\n",
    "        query = hidden[0].permute(1, 0, 2)  # Use hidden state for attention query, hidden[0] is the hidden state\n",
    "        context, attn_weights = self.attention(query, encoder_outputs)\n",
    "\n",
    "        input_lstm = torch.cat((embedded, context), dim=2)\n",
    "        output, hidden = self.lstm(input_lstm, hidden)  # LSTM returns (output, (hidden, cell))\n",
    "\n",
    "        output = self.out(output)\n",
    "        return output, hidden, attn_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do it!\n",
    "hidden_size = 256\n",
    "batch_size = 32\n",
    "\n",
    "input_lang, output_lang, train_dataloader, valid_dataloader , test_dataloader = get_dataloader(batch_size)\n",
    "\n",
    "# for i, (input_tensor, target_tensor) in enumerate(train_dataloader):\n",
    "#     print(input_tensor[0], target_tensor[0])\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "\n",
    "def evaluate_epoch(dataloader, encoder, decoder, criterion):\n",
    "\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            input_tensor, target_tensor = data\n",
    "\n",
    "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "            decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "            loss = criterion(\n",
    "                decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "                target_tensor.view(-1)\n",
    "            )\n",
    "\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "\n",
    "def train_epoch(dataloader, valid_dataloader, encoder, decoder,\n",
    "                encoder_optimizer, decoder_optimizer, criterion, epoch, best_valid_loss, ckpt_path = 'checkpoint.pth'):\n",
    "    total_loss = 0\n",
    "    \n",
    "    # 训练模式\n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    for data in tqdm(dataloader, desc=f\"Training Epoch {epoch}\"):\n",
    "        input_tensor, target_tensor = data\n",
    "\n",
    "        encoder_optimizer.zero_grad()\n",
    "        decoder_optimizer.zero_grad()\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, _, _ = decoder(encoder_outputs, encoder_hidden, target_tensor)\n",
    "\n",
    "        loss = criterion(\n",
    "            decoder_outputs.view(-1, decoder_outputs.size(-1)),\n",
    "            target_tensor.view(-1)\n",
    "        )\n",
    "        loss.backward()\n",
    "\n",
    "        encoder_optimizer.step()\n",
    "        decoder_optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # 计算训练集损失\n",
    "    avg_train_loss = total_loss / len(dataloader)\n",
    "\n",
    "    # 评估验证集损失\n",
    "    avg_valid_loss = evaluate_epoch(valid_dataloader, encoder, decoder, criterion)\n",
    "\n",
    "    print(f'Epoch {epoch}, Train Loss: {avg_train_loss:.4f}, Valid Loss: {avg_valid_loss:.4f}')\n",
    "\n",
    "    if avg_valid_loss < best_valid_loss:\n",
    "        best_valid_loss = avg_valid_loss\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'encoder_state_dict': encoder.state_dict(),\n",
    "            'decoder_state_dict': decoder.state_dict(),\n",
    "            'encoder_optimizer_state_dict': encoder_optimizer.state_dict(),\n",
    "            'decoder_optimizer_state_dict': decoder_optimizer.state_dict(),\n",
    "            'loss': best_valid_loss,\n",
    "        }, ckpt_path)\n",
    "        print(f\"Checkpoint saved at epoch {epoch} with validation loss {best_valid_loss:.4f}\")\n",
    "    return avg_train_loss, avg_valid_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import math\n",
    "\n",
    "def asMinutes(s):\n",
    "    m = math.floor(s / 60)\n",
    "    s -= m * 60\n",
    "    return '%dm %ds' % (m, s)\n",
    "\n",
    "def timeSince(since, percent):\n",
    "    now = time.time()\n",
    "    s = now - since\n",
    "    es = s / (percent)\n",
    "    rs = es - s\n",
    "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.switch_backend('agg')\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "def showPlot(points):\n",
    "    plt.figure()\n",
    "    fig, ax = plt.subplots()\n",
    "    # this locator puts ticks at regular intervals\n",
    "    loc = ticker.MultipleLocator(base=0.2)\n",
    "    ax.yaxis.set_major_locator(loc)\n",
    "    plt.plot(points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some evaluation methods\n",
    "def evaluate(encoder, decoder, sentence, input_lang, output_lang):\n",
    "    with torch.no_grad():\n",
    "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
    "\n",
    "        encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "        decoder_outputs, decoder_hidden, decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "\n",
    "        _, topi = decoder_outputs.topk(1)\n",
    "        decoded_ids = topi.squeeze()\n",
    "\n",
    "        decoded_words = []\n",
    "        for idx in decoded_ids:\n",
    "            if idx.item() == EOS_token:\n",
    "                decoded_words.append('<EOS>')\n",
    "                break\n",
    "            decoded_words.append(output_lang.index2word[idx.item()])\n",
    "    return decoded_words, decoder_attn\n",
    "\n",
    "def evaluateRandomly(encoder, decoder, n=10):\n",
    "    for i in range(n):\n",
    "        pair = random.choice(pairs)\n",
    "        print('>', pair[0])\n",
    "        print('=', pair[1])\n",
    "        output_words, _ = evaluate(encoder, decoder, pair[0], input_lang, output_lang)\n",
    "        output_sentence = ' '.join(output_words)\n",
    "        print('<', output_sentence)\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader, valid_dataloader, encoder, decoder, n_epochs, learning_rate=0.001,\n",
    "          print_every=100, plot_every=100, ckpt_path='checkpoint.pth'):\n",
    "    start = time.time()\n",
    "    plot_losses = []\n",
    "    print_loss_total = 0  # Reset every print_every\n",
    "    plot_loss_total = 0  # Reset every plot_every\n",
    "\n",
    "    encoder_optimizer = optim.Adam(encoder.parameters(), lr=learning_rate)\n",
    "    decoder_optimizer = optim.Adam(decoder.parameters(), lr=learning_rate)\n",
    "    criterion = nn.NLLLoss()\n",
    "    best_valid_loss = float('inf')\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        # 训练模型并计算训练损失\n",
    "        train_loss = train_epoch(train_dataloader, valid_dataloader, encoder, decoder,\n",
    "                                 encoder_optimizer, decoder_optimizer, criterion, epoch, best_valid_loss, ckpt_path)\n",
    "\n",
    "        print_loss_total += train_loss[0]  # 训练损失\n",
    "        plot_loss_total += train_loss[0]    # 训练损失\n",
    "        best_valid_loss = train_loss[1]\n",
    "        # 计算验证损失\n",
    "        valid_loss = train_loss[1]\n",
    "\n",
    "        if epoch % print_every == 0:\n",
    "            print_loss_avg = print_loss_total / print_every\n",
    "            print_loss_total = 0\n",
    "            print('%s (%d %d%%) Train Loss: %.4f, Valid Loss: %.4f' % (\n",
    "                timeSince(start, epoch / n_epochs),\n",
    "                epoch, epoch / n_epochs * 100, print_loss_avg, valid_loss))\n",
    "\n",
    "        if epoch % plot_every == 0:\n",
    "            plot_loss_avg = plot_loss_total / plot_every\n",
    "            plot_losses.append(plot_loss_avg)\n",
    "            plot_loss_total = 0\n",
    "\n",
    "    showPlot(plot_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)\n",
    "# train the model!\n",
    "encoder = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
    "decoder = AttnDecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
    "print(train_dataloader)\n",
    "train(train_dataloader, valid_dataloader, encoder, decoder, 10, print_every=5, plot_every=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# after training, we can evaluate the model!\n",
    "checkpoint = torch.load(\"checkpoint.pth\")\n",
    "encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "decoder.load_state_dict(checkpoint['decoder_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random evaluation\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "evaluateRandomly(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate some specific sentences\n",
    "def showAttention(input_sentence, output_words, attentions):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    cax = ax.matshow(attentions.cpu().numpy(), cmap='bone')\n",
    "    fig.colorbar(cax)\n",
    "\n",
    "    # Set up axes\n",
    "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
    "                       ['<EOS>'], rotation=90)\n",
    "    ax.set_yticklabels([''] + output_words)\n",
    "\n",
    "    # Show label at every tick\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "def evaluateAndShowAttention(input_sentence):\n",
    "    output_words, attentions = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "    print('input =', input_sentence)\n",
    "    print('output =', ' '.join(output_words))\n",
    "\n",
    "\n",
    "evaluateAndShowAttention('私の名前は愛です')\n",
    "\n",
    "evaluateAndShowAttention('昨日はお肉を食べません')\n",
    "\n",
    "evaluateAndShowAttention('いただきますよう')\n",
    "\n",
    "evaluateAndShowAttention('おはようございます')\n",
    "\n",
    "evaluateAndShowAttention('秋は好きです')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate the BLEU score\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "\n",
    "def evaluate_model_on_dataloader(encoder, decoder, input_lang, output_lang, dataloader):\n",
    "    hypotheses = []\n",
    "    references = []\n",
    "\n",
    "    for i, (input_tensor, target_tensor) in enumerate(dataloader):\n",
    "        try:\n",
    "            input_sentence = ' '.join([input_lang.index2word[idx.item()] for idx in input_tensor[0] if idx.item() != EOS_token and idx.item() != SOS_token])\n",
    "            reference_sentence = ' '.join([output_lang.index2word[idx.item()] for idx in target_tensor[0] if idx.item() != EOS_token and idx.item() != SOS_token])\n",
    "            reference_words = reference_sentence.split(' ')  \n",
    "            reference_words.append('<EOS>')  \n",
    "\n",
    "            output_words, _ = evaluate(encoder, decoder, input_sentence, input_lang, output_lang)\n",
    "            if output_words and output_words != ['<EOS>']: \n",
    "                hypotheses.append(output_words)\n",
    "                references.append(reference_words) \n",
    "            else:\n",
    "                print(f\"Warning: No valid output generated for input index {i}, skipping reference.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing pair at index {i}: {e}\")\n",
    "\n",
    "    assert len(hypotheses) == len(references), f\"Hypotheses length {len(hypotheses)} does not match references length {len(references)}\"\n",
    "\n",
    "    if hypotheses: \n",
    "        bleu_score = corpus_bleu([[ref] for ref in references], hypotheses)\n",
    "        return bleu_score\n",
    "    else:\n",
    "        print(\"No valid hypotheses generated, cannot calculate BLEU score.\")\n",
    "        return 0.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [train_dataloader, valid_dataloader, test_dataloader]\n",
    "for i, dataloader in enumerate(datasets):\n",
    "    print(f\"Evaluating on {['train', 'valid', 'test'][i]} dataset\")\n",
    "    bleu_score = evaluate_model_on_dataloader(encoder, decoder, input_lang, output_lang, dataloader)\n",
    "    print(f\"BLEU score: {bleu_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_perplexity(encoder, decoder, dataloader, input_lang, output_lang):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    total_loss = 0.0\n",
    "    total_tokens = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (input_tensor, target_tensor) in enumerate(dataloader):\n",
    "            \n",
    "            input_tensor = input_tensor.to(device)\n",
    "            target_tensor = target_tensor.to(device)\n",
    "\n",
    "            encoder_outputs, encoder_hidden = encoder(input_tensor)\n",
    "            decoder_input = target_tensor[:, :-1]  \n",
    "            target_tokens = target_tensor[:, 1:]  \n",
    "\n",
    "            decoder_input = decoder_input.float()\n",
    "            target_tokens = target_tokens.float()\n",
    "            # 计算交叉熵损失\n",
    "            decoder_outputs, decoer_hidden,decoder_attn = decoder(encoder_outputs, encoder_hidden)\n",
    "            \n",
    "            loss = F.cross_entropy(decoder_outputs.reshape(-1, decoder_outputs.size(-1)), target_tensor.reshape(-1), reduction='mean')\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            total_tokens += 1\n",
    "            \n",
    "        avg_loss = total_loss / total_tokens  # 计算平均损失\n",
    "\n",
    "        perplexity = torch.exp(torch.tensor(avg_loss))  # 计算困惑度\n",
    "\n",
    "    return perplexity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [train_dataloader, valid_dataloader, test_dataloader]\n",
    "\n",
    "for i, dataloader in enumerate(datasets):\n",
    "    print(f\"Evaluating on {['train', 'valid', 'test'][i]} dataset\")\n",
    "    bleu_score = calculate_perplexity(encoder, decoder,  dataloader,input_lang,output_lang)\n",
    "    print(f\"PPL score: {bleu_score:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "np-nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
